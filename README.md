# LLM-Detect-AI-Generated-Text

## Introduction
Here we are given essays, some of them are written by around 10th or 11th standard students and other generated by large language models (LLMs), our task is to assign probability of an essay being generated by an LLM.

## Data Preparation
As our data was initially imbalanced, it had many human written essays compared to generated ones. We found data on Kaggle only which helped us to make our data balanced from (DAIGT-proper-train-dataset). We added this dataset to test essays.

## Text Data Preprocessing
Here we did Tokenization and padding of the data. Tokenization converts raw text into a structured format that a machine learning model can understand. It provides a way to represent textual information as a sequence of discrete elements. By tokenizing text, we create a vocabulary where each unique word is assigned a numerical index. This numerical representation facilitates feature extraction and allows the model to operate on a structured input.
Neural networks require input sequences of the same length. Padding ensures that all sequences have the same length, facilitating batch processing during training.

## Model Architecture
Here we used Convolution Neural Network Architecture. Then we did dropout and Batch normalization for regularization. Here we use optimal dropout rates and used batch normalization which aided in stabilizing and accelerating the training process.

## Model Training 
I used Adamax optimizer here, it combines the benefits of both AdaGrad and RMSProp, providing adaptive learning rates and efficient updates. I used binary_crossentropy for loss function, Binary Cross entropy is a common choice for binary classification problems. It measures the difference between predicted probabilities and true binary labels. This loss function can be used only when the task involves classifying each sample into one of two classes (0 or 1).
We plotted the training and validation accuracy values over each epoch. It helped assess how well the model is learning from the training data and how well it generalizes to unseen validation data.
Loss plot shows the training and validation loss values over each epoch. The loss represents the difference between predicted and true values. Monitoring loss helps understand how well the model is minimizing errors during training.

## Validation Metrics
Finally, we interpret accuracy through confusion matrix and classification report. Also, I calculated precision, recall and F-1 score in order to see the distribution of type-1 and type-2 error.
